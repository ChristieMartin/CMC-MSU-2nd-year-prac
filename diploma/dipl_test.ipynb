{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom.minidom import (parse, Element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filename = 'Tselina'\n",
    "#corpus_filename = 'pedagogika'\n",
    "#corpus_filename = 'sirija'\n",
    "\n",
    "file_ext = '.xml'\n",
    "doc = parse(corpus_filename + file_ext)\n",
    "sentences = doc.getElementsByTagName('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Количество предложений в корпусе = 189'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Количество предложений в корпусе = {len(sentences)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, sent: Element):\n",
    "        self.id = sent.getAttribute('ID')\n",
    "        self.words = []\n",
    "        \n",
    "        self.parseWords(sent.getElementsByTagName('W'))\n",
    "        \n",
    "    def parseWords(self, rawWords):\n",
    "        \n",
    "        self.wordMap = {}\n",
    "        \n",
    "        for rawWord in rawWords:\n",
    "            \n",
    "            word = Word(rawWord, self.id)\n",
    "            \n",
    "            self.words.append(word)\n",
    "            \n",
    "            if word.dom == '_root':\n",
    "                self.rootWord = word\n",
    "                \n",
    "            if word.dom in self.wordMap.keys():\n",
    "                self.wordMap[word.dom].append(word)\n",
    "            else:\n",
    "                self.wordMap[word.dom] = [word]\n",
    "\n",
    "        for word in self.words:\n",
    "            if word.id in self.wordMap.keys():\n",
    "                for d in self.wordMap[word.id]:\n",
    "                    word.addWord(d)\n",
    "    def printGram(self):\n",
    "        self.rootWord.printGram(True) \n",
    "        \n",
    "        \n",
    "class Word:\n",
    "    def __init__(self, w: Element, sentId):\n",
    "        self.dom = w.getAttribute('DOM')\n",
    "        self.feat = w.getAttribute('FEAT')\n",
    "        self.id = w.getAttribute('ID')\n",
    "        self.lemma = w.getAttribute('LEMMA')\n",
    "        self.link = w.getAttribute('LINK')\n",
    "        \n",
    "        v = \" \".join(t.nodeValue for t in w.childNodes if t.nodeType == t.TEXT_NODE).lower().strip()\n",
    "        if len(v) != 0 and v[-1] == '.':\n",
    "            v = v[:-1]\n",
    "        if v != '':\n",
    "            if self.feat in d.keys():\n",
    "                d[self.feat].add(v)\n",
    "            else:\n",
    "                d[self.feat] = {v}\n",
    "        \n",
    "        \n",
    "        self.sentId = sentId\n",
    "        \n",
    "        self.connectedWords = []\n",
    "        \n",
    "    def addWord(self, w):\n",
    "        self.connectedWords.append(w)\n",
    "        \n",
    "    def printGram(self, checkProj):\n",
    "        if checkProj:\n",
    "            t = checkProjectivity(self, {}, 0)\n",
    "            if not t:\n",
    "                # закомментировать return, чтобы такие предложения не пропускались\n",
    "                #print(f'\\nNOT PROJECTIVE, {self.sentId}\\n')\n",
    "                return\n",
    "        print('F{' + f'[{self.feat}' + ']}->', end = '')\n",
    "        \n",
    "        if self.connectedWords == []:\n",
    "            print(f'[{self.feat}]\\n', end = '')\n",
    "            \n",
    "        else:\n",
    "            t = False\n",
    "            for w in self.connectedWords:\n",
    "                if not t and (int(w.id) > int(self.id)): \n",
    "                    print(f'[{self.feat}]', end = '')\n",
    "                    t = True\n",
    "                    \n",
    "                print(';D{[' + f'{w.feat}' + '], ' + f'{w.link}' + '};', end = '')\n",
    "            if not t:\n",
    "                print(f'[{self.feat}]', end = '')\n",
    "            for w in self.connectedWords:\n",
    "                print('\\nD{[' + f'{w.feat}], {w.link}' + '}->F{[' + f'{w.feat}' + ']}')\n",
    "                #print() # раскомментировать и закомментировать предыдущую, если не хотим печатать D(t, s) -> F(t)\n",
    "                w.printGram(False)\n",
    "                \n",
    "def checkProjectivity(rootWord, seenWordsIds, floor):\n",
    "    if floor not in seenWordsIds.keys():\n",
    "        seenWordsIds[floor] = []\n",
    "    if any(map(lambda x: int(x) > int(rootWord.id), seenWordsIds[floor])):\n",
    "        return False\n",
    "    seenWordsIds[floor].append(rootWord.id)\n",
    "    if rootWord.connectedWords == []:\n",
    "        return True\n",
    "    t = True\n",
    "    for w in rootWord.connectedWords:\n",
    "        t = t and checkProjectivity(w, seenWordsIds, floor + 1)\n",
    "        if not t:\n",
    "            return t\n",
    "    return t   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSentences():\n",
    "    for sent in sentences:\n",
    "        parseSentence(sent)\n",
    "\n",
    "def parseSentence(sent: Element):\n",
    "    sentence = Sentence(sent)\n",
    "    sentence.printGram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "filename = corpus_filename + '_' + 'gram.out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для теста\n",
    "# печатает грамматику одного предложения\n",
    "\n",
    "sentence_id = 119 #id предложения в корпусе <S ID = 'sentence_id'>\n",
    "\n",
    "filename = 'test/test.out'\n",
    "filename_sorted = 'test/test_sorted.out'\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "f = open(filename, 'w', encoding = 'utf-16')\n",
    "sys.stdout = f\n",
    "\n",
    "sentence = Sentence(sentences[sentence_id - 1])\n",
    "sentence.printGram()\n",
    "\n",
    "sys.stdout = orig_stdout\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_sorted = corpus_filename + '_' + 'gram_sorted.out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSortedNoDuplicatesFile(fname, fname_sorted):\n",
    "    lines_seen = set()\n",
    "    with open(fname, 'r', encoding = 'utf-16') as r:\n",
    "        with open(fname_sorted, 'w', encoding = 'utf-16') as f:\n",
    "            for line_orig in sorted(r):\n",
    "                if line_orig not in lines_seen:\n",
    "                    line = str(line_orig.strip())\n",
    "\n",
    "                    if len(line) != 0 and line[-1] == ';':\n",
    "                        line = line[:-1].strip()\n",
    "                    line = line.replace(';;', ';')\n",
    "                    line = line.replace('->;', '->')\n",
    "                    line += '\\n'  \n",
    "                    if line == '\\n':\n",
    "                        continue\n",
    "                    print(line, end = '', file = f)\n",
    "                    lines_seen.add(line_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "printSortedNoDuplicatesFile(filename, filename_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDictionary(d, fname):\n",
    "    with open(fname, 'w', encoding='utf-16') as f:\n",
    "        for key, value in d.items():\n",
    "            s = ' | '.join(value)\n",
    "            print(f'[{key}] = {s}', file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "printDictionary(d, f'{corpus_filename}_dict.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_gram_map = {}\n",
    "f_gram_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToMap(mapName, key, value):\n",
    "    if key in mapName.keys():\n",
    "        mapName[key].append(value)\n",
    "    else:\n",
    "        mapName[key] = [value]\n",
    "\n",
    "def mapGrammar(filename):\n",
    "    with open(filename, 'r', encoding = 'utf-16') as r:\n",
    "        for line in r:\n",
    "            s = line.split('->')\n",
    "            p = s[0].strip()\n",
    "            w = s[1].strip().replace('\\n', '')\n",
    "            if (p[0] == 'D'):\n",
    "                addToMap(d_gram_map, p, w)\n",
    "            else:\n",
    "                addToMap(f_gram_map, p, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printGrammarMap(mapName, filename_min):\n",
    "    with open(filename_min, 'w', encoding = 'utf-16') as f:\n",
    "        for key, value in mapName.items():\n",
    "            s = '|'.join(value)\n",
    "            s = ';'.join(s.split(';'))\n",
    "            print(key, '->', s, file = f, sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dMapFirstStep():\n",
    "    for key, value in d_gram_map.items():\n",
    "        d_gram_map[key] = f_gram_map[d_gram_map[key][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeRecursion(d_gram_map):\n",
    "    seen = []\n",
    "    t = True\n",
    "    while t:\n",
    "        t = False\n",
    "        for key, value in d_gram_map.items():\n",
    "            if 'D{' not in str(value) and key not in seen:\n",
    "                t = True\n",
    "                seen.append(key)\n",
    "                for k, v in d_gram_map.items():\n",
    "                    l = []\n",
    "                    if key in str(v):\n",
    "                        for item in v:\n",
    "                            for val in value:\n",
    "                                l.append(item.replace(key, val))\n",
    "                    if l != []:\n",
    "                        d_gram_map[k] = l\n",
    "                        \n",
    "    return d_gram_map\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_gram_map = {}\n",
    "f_gram_map = {}\n",
    "\n",
    "\n",
    "mapGrammar('test/test_sorted.out')\n",
    "    \n",
    "dMapFirstStep()\n",
    "\n",
    "new_map = removeRecursion(copy.deepcopy(d_gram_map))\n",
    "\n",
    "printGrammarMap(new_map, 'test/test_gram_min.out')\n",
    "printSortedNoDuplicatesFile('test/test_gram_min.out', 'test/test_gram_min_sorted.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTNNode:\n",
    "    def __init__(self, s, i, isEnd = False):\n",
    "        self.s = s\n",
    "        self.i = i\n",
    "        self.isEnd = isEnd\n",
    "        self.connected = None\n",
    "    def connect(self, node):\n",
    "        self.connected = node\n",
    "    def printNode(self):\n",
    "        s = f'${self.s}' if self.s[0] == 'D' else self.s\n",
    "        if self.isEnd:\n",
    "            print(s, '*', end = '')\n",
    "            print(';')\n",
    "        else:\n",
    "            print(s, self.i, end = '')\n",
    "            print(';')\n",
    "            \n",
    "    def printNodes(self, printItself = False):\n",
    "        if printItself:\n",
    "            self.printNode()\n",
    "        if not self.isEnd:\n",
    "            print(f'{self.i}:')\n",
    "        if self.connected != None:\n",
    "            self.connected.printNodes(printItself = True)\n",
    "        \n",
    "\n",
    "def toRTN(new_map, fname):\n",
    "    orig_stdout = sys.stdout\n",
    "    f = open(fname, 'w', encoding = 'utf-16')\n",
    "    sys.stdout = f\n",
    "    for key in new_map.keys():\n",
    "        l = new_map[key]\n",
    "        new_l = []\n",
    "        m_len = 0\n",
    "        for item in l:\n",
    "            p = item.split(';')\n",
    "            if len(p) > m_len:\n",
    "                m_len = len(p)\n",
    "            new_l.append(p)\n",
    "        print(f'${key}')\n",
    "        print('(')\n",
    "        j = 0\n",
    "        k = 0\n",
    "        t = 0\n",
    "        m = {}\n",
    "        last_ones = []\n",
    "        for items in new_l:\n",
    "            last = None\n",
    "            if k == 0:\n",
    "                k = len(items) - 1\n",
    "            else:\n",
    "                k += t\n",
    "            j = k\n",
    "            t = 0\n",
    "            \n",
    "            for i, item in enumerate(reversed(items)):\n",
    "                if i == 0:\n",
    "                    r = RTNNode(item, 0, isEnd = i == 0)\n",
    "                    last = r\n",
    "                    continue\n",
    "                r = RTNNode(item, j, isEnd = i == 0)\n",
    "                j -= 1\n",
    "                t += 1\n",
    "            \n",
    "                if last != None:\n",
    "                    r.connect(last)\n",
    "                last = r\n",
    "                \n",
    "            last_ones.append(last)\n",
    "                \n",
    "        print('0:')\n",
    "        for i in last_ones:\n",
    "            i.printNode()\n",
    "        for i in last_ones:\n",
    "            i.printNodes()        \n",
    "        print(')')\n",
    "        print()\n",
    "    sys.stdout = orig_stdout\n",
    "    f.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_map = {\n",
    "    'S': ['D1;D2;D3;t;D4;D5'],\n",
    "    'S1': ['a;D1', 'D2;D1']\n",
    "}\n",
    "\n",
    "t_new_map = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t_new_map:\n",
    "    toRTN(test_new_map, f't_new_map.out')\n",
    "else:\n",
    "    toRTN(new_map, f'test/test_rtn.out')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
